{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvUZKx0MCeBnjD928Smyvu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrgf/Projeto_tcc/blob/main/Modelo_def_tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento Modelo #1 Tcc: Criação de modelo capaz de analisar e desenvolver testes simples Web\n",
        "\n",
        "Etapa #1: Requerimentos do Modelo - Modelo deve analisar uma pagina web e identificar elementos comuns, como caixas de texto e botões e desenvolver testes para esses elementos\n",
        "\n",
        "Etapa #2: Coletar e rotular dados atraves de uma coleta dados de treinamento que possam ser usados ​​para treinar o modelo. Esses dados podem incluir screenshots de páginas da web, informações sobre os elementos da página e testes de exemplo que já foram escritos.\n",
        "\n",
        "Etapa #3: Treinar esse modelo para poder conseguir prever se um novo 'teste' sem uma label é valido ou não\n",
        "\n",
        "Site Exemplo: http://saucedemo.com/ https://testpages.herokuapp.com/styled/index.html https://theuselessweb.com/"
      ],
      "metadata": {
        "id": "-TRD6egfjSZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "ZTetLn18jWOB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYf3_9ffjMsj"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import random\n",
        "import csv\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates the datasets"
      ],
      "metadata": {
        "id": "9_EgwohfuW7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_log_file(log_file, subset_ratio):\n",
        "    modified_file = \"modified_file.log\" # Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "    subset_file = \"subset_file.log\" # Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "    output_file_1 = \"dataset_1.csv\" # Arquivo de saída para dataset_1 dividido em 6 categorias e com labels.\n",
        "    output_file_2 = \"dataset_2.csv\" # Arquivo de saída para dataset_2 dividido em 6 categorias.\n",
        "\n",
        "    urls = [\n",
        "        \"https://www.saucedemo.com/cart.html\",\n",
        "        \"https://www.saucedemo.com/inventory-item.html\",\n",
        "        \"https://www.saucedemo.com/inventory.html\",\n",
        "        \"https://www.saucedemo.com/login.html\",\n",
        "        \"https://www.saucedemo.com/\",\n",
        "        \"https://www.saucedemo.com/inventory-item.html\"\n",
        "    ]\n",
        "\n",
        "    modified_lines = []\n",
        "    subset_indexes = []\n",
        "\n",
        "    # Abre o arquivo de log original\n",
        "    with open(log_file, \"r\") as f_log:\n",
        "        lines = f_log.readlines()\n",
        "        subset_size = int(len(lines) * subset_ratio)\n",
        "        subset_indexes = random.sample(range(len(lines)), subset_size)\n",
        "\n",
        "        # Itera pelas linhas do arquivo de log\n",
        "        for i, line in enumerate(lines):\n",
        "            # Verifica se o índice da linha está no conjunto de índices do subconjunto\n",
        "            if i in subset_indexes:\n",
        "                parts = line.split()\n",
        "                # Se o numero aleatorio cair em 10, o usuairo muda apenas o numero de elementos na linha\n",
        "                mod_type = random.randint(1, 10)\n",
        "                if mod_type == 10:\n",
        "                    num_elements = random.randint(1, 150)\n",
        "                    parts[4] = str(num_elements)\n",
        "                else:\n",
        "                    # Se não, ele muda dois parametros da linha, para erros aleatorios.\n",
        "                    num_elements = random.randint(1, 150)\n",
        "                    parts[4] = str(num_elements)\n",
        "                    page = random.choice(urls)\n",
        "                    parts[-1] = page\n",
        "                new_line = \" \".join(parts) + \"\\n\"\n",
        "                modified_lines.append(new_line)\n",
        "            else:\n",
        "                modified_lines.append(line)\n",
        "\n",
        "    # Escreve as linhas modificadas no arquivo de subconjunto\n",
        "    with open(subset_file, \"w\") as f_subset:\n",
        "        f_subset.writelines(modified_lines[i] for i in subset_indexes)\n",
        "\n",
        "    # Escreve todas as linhas modificadas no arquivo modificado\n",
        "    with open(modified_file, \"w\") as f:\n",
        "        f.writelines(modified_lines)\n",
        "\n",
        "    # Abre o arquivo de subconjunto em modo de leitura\n",
        "    with open(subset_file, \"r\") as f:\n",
        "        subset_lines = f.readlines()\n",
        "        subset_lines = [line.strip().split() for line in subset_lines]\n",
        "\n",
        "    # Abre o arquivo de saída para dataset_1 em modo de escrita\n",
        "    with open(output_file_1, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escreve a linha de cabeçalho com as categorias, incluindo a categoria adicional 'label'\n",
        "        writer.writerow(['time', 'date', 'element', 'action', 'n_elem', 'url', 'label'])\n",
        "\n",
        "        # Abre o arquivo original em modo de leitura\n",
        "        with open(modified_file, 'r') as f:\n",
        "            modified_lines = f.readlines()\n",
        "            modified_lines = [line.strip().split() for line in modified_lines]\n",
        "\n",
        "            # Itera pelas linhas do arquivo original\n",
        "            for line in modified_lines:\n",
        "                # Verifica se a linha está presente no subconjunto\n",
        "                if line[:5] in [l[:5] for l in subset_lines]:\n",
        "                    # Escreve a linha com a categoria 'fault' adicionada\n",
        "                    writer.writerow(line + ['fault'])\n",
        "                else:\n",
        "                    # Escreve a linha com a categoria 'valid' adicionada\n",
        "                    writer.writerow(line + ['valid'])\n",
        "\n",
        "    # Abre o arquivo de saída para dataset_2 em modo de escrita\n",
        "    with open(output_file_2, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escreve a linha de cabeçalho com as categorias\n",
        "        writer.writerow(['time', 'date', 'element', 'action', 'n_elem', 'url'])\n",
        "\n",
        "        # Abre o arquivo original em modo de leitura\n",
        "        with open(modified_file, 'r') as f:\n",
        "            modified_lines = f.readlines()\n",
        "            modified_lines = [line.strip().split() for line in modified_lines]\n",
        "\n",
        "            # Itera pelas linhas do arquivo original\n",
        "            for line in modified_lines:\n",
        "                # Escreve as categorias no arquivo de saída como uma nova linha\n",
        "                writer.writerow(line)\n",
        "\n",
        "log_file_1 = \"dic.log\"\n",
        "subset_ratio_1 = 0.4\n",
        "\n",
        "log_file_2 = \"dic_2.log\"\n",
        "subset_ratio_2 = 0.2\n",
        "\n",
        "process_log_file(log_file_1, subset_ratio_1)\n",
        "process_log_file(log_file_2, subset_ratio_2)\n"
      ],
      "metadata": {
        "id": "DzwBmQrApSzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "trains the dataset with diferent parameters"
      ],
      "metadata": {
        "id": "UynMt7Z1ubK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para treinar um modelo e retornar as métricas de avaliação\n",
        "def train_model(X_train, X_test, y_train, y_test, model):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    return accuracy, precision, recall, roc_auc, conf_matrix\n",
        "\n",
        "# Carregar os dados do arquivo 'dataset_1.csv' usando o pandas\n",
        "data = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remover as colunas 'date' e 'time' dos dados, pois não são relevantes\n",
        "data = data.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Converter colunas categóricas em valores numéricos nos dados\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)\n",
        "\n",
        "# Aplicar codificação one-hot na coluna 'action' nos dados\n",
        "data = pd.get_dummies(data, columns=['action'])\n",
        "\n",
        "# Converter a coluna 'url' em valores numéricos nos dados\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)\n",
        "\n",
        "# Substituir os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' dos dados\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Separar as features (X) e os rótulos (y) dos dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinar o modelo de Regressão Logística isoladamente\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_accuracy, lr_precision, lr_recall, lr_roc_auc, lr_conf_matrix = train_model(X_train, X_test, y_train, y_test, lr_model)\n",
        "\n",
        "# Treinar o modelo de Regressão Logística com Árvore de Decisão\n",
        "lr_dt_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), algorithm='SAMME', random_state=42)\n",
        "lr_dt_accuracy, lr_dt_precision, lr_dt_recall, lr_dt_roc_auc, lr_dt_conf_matrix = train_model(X_train, X_test, y_train, y_test, lr_dt_model)\n",
        "\n",
        "# Treinar o modelo AdaBoost isoladamente\n",
        "ada_model = AdaBoostClassifier(random_state=42)\n",
        "ada_accuracy, ada_precision, ada_recall, ada_roc_auc, ada_conf_matrix = train_model(X_train, X_test, y_train, y_test, ada_model)\n",
        "\n",
        "# Treinar o modelo AdaBoost com Árvore de Decisão\n",
        "ada_dt_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), algorithm='SAMME', random_state=42)\n",
        "ada_dt_accuracy, ada_dt_precision, ada_dt_recall, ada_dt_roc_auc, ada_dt_conf_matrix = train_model(X_train, X_test, y_train, y_test, ada_dt_model)\n",
        "\n",
        "# Criar uma tabela para comparar os resultados\n",
        "results = pd.DataFrame({\n",
        "    'Modelo': ['Regressão Logística', 'Regressão Logística + Árvore de Decisão', 'AdaBoost', 'AdaBoost + Árvore de Decisão'],\n",
        "    'Acurácia': [lr_accuracy, lr_dt_accuracy, ada_accuracy, ada_dt_accuracy],\n",
        "    'Precisão': [lr_precision, lr_dt_precision, ada_precision, ada_dt_precision],\n",
        "    'Recall': [lr_recall, lr_dt_recall, ada_recall, ada_dt_recall],\n",
        "    'ROC AUC': [lr_roc_auc, lr_dt_roc_auc, ada_roc_auc, ada_dt_roc_auc],\n",
        "    'Matriz de Confusão': [lr_conf_matrix, lr_dt_conf_matrix, ada_conf_matrix, ada_dt_conf_matrix]\n",
        "})\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13m0TVvJuifB",
        "outputId": "b2ab9baf-0067-4cbf-af75-2dfbc9f618c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 Model  Accuracy  Precision    Recall  \\\n",
            "0                  Logistic Regression  0.846154   0.875000  0.954545   \n",
            "1  Logistic Regression + Decision Tree  0.961538   0.956522  1.000000   \n",
            "2                             AdaBoost  0.961538   0.956522  1.000000   \n",
            "3             AdaBoost + Decision Tree  0.961538   0.956522  1.000000   \n",
            "\n",
            "    ROC AUC   Confusion Matrix  \n",
            "0  0.602273  [[1, 3], [1, 21]]  \n",
            "1  0.875000  [[3, 1], [0, 22]]  \n",
            "2  0.875000  [[3, 1], [0, 22]]  \n",
            "3  0.875000  [[3, 1], [0, 22]]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "try to guess from the dataset_2 that dont have labels"
      ],
      "metadata": {
        "id": "fXaf22y-64Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_logistic():\n",
        "    # Carregar os dados de treinamento do arquivo 'dataset_1.csv'\n",
        "    data_train = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "    # Remover as colunas 'date' e 'time' dos dados de treinamento\n",
        "    data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Converter colunas categóricas em valores numéricos nos dados de treinamento\n",
        "    for col in data_train.columns:\n",
        "        if data_train[col].dtype == object:\n",
        "            data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)\n",
        "\n",
        "    # Aplicar codificação one-hot na coluna 'action' dos dados de treinamento\n",
        "    data_train = pd.get_dummies(data_train, columns=['action'])\n",
        "\n",
        "    # Converter a coluna 'url' em valores numéricos nos dados de treinamento\n",
        "    data_train['url'] = pd.Categorical(data_train['url']).codes.astype(float)\n",
        "\n",
        "    # Substituir 'valid' por 0 e 'fault' por 1 na coluna 'label' dos dados de treinamento\n",
        "    data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "    # Separar as features (X_train) e o rótulo (y_train) dos dados de treinamento\n",
        "    X_train = data_train.drop(['label'], axis=1)\n",
        "    y_train = data_train['label']\n",
        "\n",
        "    # Criar um modelo de regressão logística e ajustá-lo aos dados de treinamento\n",
        "    model = LogisticRegression(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Carregar os dados de teste do arquivo 'dataset_2.csv'\n",
        "    data_test = pd.read_csv('dataset_2.csv')\n",
        "\n",
        "    # Remover as colunas 'date' e 'time' dos dados de teste\n",
        "    data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Converter colunas categóricas em valores numéricos nos dados de teste\n",
        "    for col in data_test.columns:\n",
        "        if data_test[col].dtype == object:\n",
        "            data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)\n",
        "\n",
        "    # Aplicar codificação one-hot na coluna 'action' dos dados de teste\n",
        "    data_test = pd.get_dummies(data_test, columns=['action'])\n",
        "\n",
        "    # Converter a coluna 'url' em valores numéricos nos dados de teste\n",
        "    data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)\n",
        "\n",
        "    # Fazer previsões para os dados de teste usando o modelo treinado\n",
        "    labels_predicted = model.predict(data_test)\n",
        "\n",
        "    # Calcular a acurácia da previsão considerando rótulos 'valid' (0)\n",
        "    accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train_and_predict_adaboost():\n",
        "    # Carregar os dados de treinamento do arquivo 'dataset_1.csv'\n",
        "    data_train = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "    # Remover as colunas 'date' e 'time' dos dados de treinamento\n",
        "    data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Substituir 'valid' por 0 e 'fault' por 1 na coluna 'label' dos dados de treinamento\n",
        "    data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "    # Aplicar codificação one-hot nas colunas 'element', 'action' e 'url' dos dados de treinamento\n",
        "    data_train = pd.get_dummies(data_train, columns=['element', 'action', 'url'])\n",
        "\n",
        "    # Separar as features (X_train) e o rótulo (y_train) dos dados de treinamento\n",
        "    X_train = data_train.drop(['label'], axis=1)\n",
        "    y_train = data_train['label']\n",
        "\n",
        "    # Definir o estimador base como DecisionTreeClassifier e criar um modelo AdaBoostClassifier\n",
        "    base_estimator = DecisionTreeClassifier()\n",
        "    model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')\n",
        "\n",
        "    # Treinar o modelo usando os dados de treinamento\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Carregar os dados de teste do arquivo 'dataset_2.csv'\n",
        "    data_test = pd.read_csv('dataset_2.csv')\n",
        "\n",
        "    # Remover as colunas 'date' e 'time' dos dados de teste\n",
        "    data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Aplicar codificação one-hot nas colunas 'element', 'action' e 'url' dos dados de teste\n",
        "    data_test = pd.get_dummies(data_test, columns=['element', 'action', 'url'])\n",
        "\n",
        "    # Reindexar as colunas dos dados de teste para corresponder às colunas dos dados de treinamento, preenchendo com zeros\n",
        "    data_test = data_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "    # Fazer previsões para os dados de teste\n",
        "    labels_predicted = model.predict(data_test)\n",
        "\n",
        "    # Calcular a acurácia do modelo considerando rótulos 'valid' (0)\n",
        "    accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Treinar e prever usando Regressão Logística\n",
        "accuracy_logistic = train_and_predict_logistic()\n",
        "\n",
        "# Treinar e prever usando AdaBoost com Decision Tree\n",
        "accuracy_adaboost = train_and_predict_adaboost()\n",
        "\n",
        "# Criar uma tabela para comparar as acurácias\n",
        "table = [[\"Modelo\", \"Acurácia\"],\n",
        "         [\"Regressão Logística\", f\"{accuracy_logistic:.2f}%\"],\n",
        "         [\"AdaBoost com Decision Tree\", f\"{accuracy_adaboost:.2f}%\"]]\n",
        "\n",
        "# Exibir a tabela\n",
        "print(tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23DCFX0e7A9o",
        "outputId": "ac2f8067-989d-49fe-c783-67109e30261e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═════════════════════════════╤════════════╕\n",
            "│ Model                       │ Accuracy   │\n",
            "╞═════════════════════════════╪════════════╡\n",
            "│ Logistic Regression         │ 3.97%      │\n",
            "├─────────────────────────────┼────────────┤\n",
            "│ AdaBoost with Decision Tree │ 80.16%     │\n",
            "╘═════════════════════════════╧════════════╛\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}