{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0LhWBhjpWteYyPNhu/+Pj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrgf/Projeto_tcc/blob/main/Modelo_tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento Modelo #1 Tcc: Criação de modelo capaz de analisar e desenvolver testes simples Web\n",
        "\n",
        "Etapa #1: Requerimentos do Modelo - Modelo deve analisar uma pagina web e identificar elementos comuns, como caixas de texto e botões e desenvolver testes para esses elementos\n",
        "\n",
        "Etapa #2: Coletar e rotular dados atraves de uma coleta dados de treinamento que possam ser usados ​​para treinar o modelo. Esses dados podem incluir screenshots de páginas da web, informações sobre os elementos da página e testes de exemplo que já foram escritos.\n",
        "\n",
        "Etapa #3: Treinar esse modelo para poder conseguir prever se um novo 'teste' sem uma label é valido ou não\n",
        "\n",
        "Site Exemplo: http://saucedemo.com/ https://testpages.herokuapp.com/styled/index.html https://theuselessweb.com/"
      ],
      "metadata": {
        "id": "-TRD6egfjSZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "ZTetLn18jWOB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYf3_9ffjMsj"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import random\n",
        "import csv\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As abordagens serão apresentadas, inicialmente, como foram feitas passo a passo, apos isso existe um codigo unico que fara, as analises e printara uma tabela com os resultados."
      ],
      "metadata": {
        "id": "SjIiFEjw1L94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo para inserir porcentagem de erros em um arquivo de log, gerando dois tipos de arquivos, um modified que representam o arquivo com os erros inseridos e o subset que é o arquivo que contem os erros que foram gerados e inseridos no outro documento\n"
      ],
      "metadata": {
        "id": "Nj089Umz1bII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "log_file = Arquivo de log original.\n",
        "modified_file = Arquivo original contendo linhas alteradas.\n",
        "\\\n",
        "subset_file= = Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "\\\n",
        "subset_ratio = Variavel contendo a porcentagem de erros que serão adicionadas no arquivo.\n",
        "\\\n",
        "element_ratio = Porcentagem de chance de mudar apenas um dos parametros"
      ],
      "metadata": {
        "id": "4EqT5lrw2TGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "log_file = \"dic_mod.log\" # Arquivo original com os testes do website\n",
        "modified_file = \"modified_file.log\" # Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "subset_file = \"subset_file.log\" # Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "subset_ratio = 0.4 # porcentagem de erros que o usuario quer no arquivo\n",
        "element_ratio = 0.2 # Porcentagem de chance de mudar apenas um dos parametros\n",
        "\n",
        "urls = [\n",
        "    \"https://www.saucedemo.com/cart.html\",\n",
        "    \"https://www.saucedemo.com/inventory-item.html\",\n",
        "    \"https://www.saucedemo.com/inventory.html\",\n",
        "    \"https://www.saucedemo.com/login.html\",\n",
        "    \"https://www.saucedemo.com/\",\n",
        "    \"https://www.saucedemo.com/inventory-item.html\"\n",
        "]\n",
        "\n",
        "modified_lines = []\n",
        "subset_indexes = []\n",
        "\n",
        "# Abre o arquivo de log original\n",
        "with open(log_file, \"r\") as f_log:\n",
        "    lines = f_log.readlines()\n",
        "    subset_size = int(len(lines) * subset_ratio)\n",
        "    subset_indexes = random.sample(range(len(lines)), subset_size)\n",
        "\n",
        "    # Itera pelas linhas do arquivo de log\n",
        "    for i, line in enumerate(lines):\n",
        "        # Verifica se o índice da linha está no conjunto de índices do subconjunto\n",
        "        if i in subset_indexes:\n",
        "            parts = line.split()\n",
        "            # Se o numero aleatorio cair em 10, o usuairo muda apenas o numero de elementos na linha\n",
        "            mod_type = random.randint(1, 10)\n",
        "            if mod_type == 10:\n",
        "                num_elements = random.randint(1, 150)\n",
        "                parts[4] = str(num_elements)\n",
        "            else:\n",
        "              # Se não, ele muda dois parametros da linha, para erros aleatorios.\n",
        "                num_elements = random.randint(1, 150)\n",
        "                parts[4] = str(num_elements)\n",
        "                page = random.choice(urls)\n",
        "                parts[-1] = page\n",
        "            new_line = \" \".join(parts) + \"\\n\"\n",
        "            modified_lines.append(new_line)\n",
        "        else:\n",
        "            modified_lines.append(line)\n",
        "\n",
        "# Escreve as linhas modificadas no arquivo de subconjunto\n",
        "with open(subset_file, \"w\") as f_subset:\n",
        "    f_subset.writelines(modified_lines[i] for i in subset_indexes)\n",
        "\n",
        "# Escreve todas as linhas modificadas no arquivo modificado\n",
        "with open(modified_file, \"w\") as f:\n",
        "    f.writelines(modified_lines)\n"
      ],
      "metadata": {
        "id": "Vr9U9mdl1f9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo abaixo utilizado para criar o dataset sem as labels de valid e fault\n",
        "\n",
        "subset_file = Arquivo original contendo linhas alteradas e não alteradas.\n",
        "\\\n",
        "original_file = Arquivo contendo os erros que serão 'labeled' fault.\n",
        "\\\n",
        "output_file = Arquivo de saida em formato de dataset."
      ],
      "metadata": {
        "id": "bssOnxxo1jRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "subset_file = 'subset_file.log' # Arquivo com tanto as linhas alteradas como as linhas não alteradas.\n",
        "original_file = 'modified_file.log' # Arquivo contendo apenas as linhas que foram alteradas.\n",
        "output_file = 'dataset_2.csv' # Arquivo de saida em formato dataset dividido em 6 categorias.\n",
        "\n",
        "categories = ['time', 'date', 'element', 'action', 'n_elem', 'url']\n",
        "\n",
        "# Abre o arquivo de saída em modo de escrita\n",
        "with open(output_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # Escreve a linha de cabeçalho com as categorias\n",
        "    writer.writerow(categories)\n",
        "\n",
        "    # Abre o arquivo original em modo de leitura\n",
        "    with open(original_file, 'r') as f:\n",
        "        original_lines = f.readlines()\n",
        "        original_lines = [line.strip() for line in original_lines]\n",
        "\n",
        "        # Itera pelas linhas do arquivo original\n",
        "        for line in original_lines:\n",
        "            # Separa as categorias da linha\n",
        "            line_categories = line.split(' ')\n",
        "\n",
        "            # Completa com strings vazias se o número de categorias for inferior a 6\n",
        "            while len(line_categories) < 6:\n",
        "                line_categories.append('')\n",
        "\n",
        "            # Escreve as categorias no arquivo de saída como uma nova linha\n",
        "            writer.writerow(line_categories)\n"
      ],
      "metadata": {
        "id": "6DidyJqg1pDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ERROS GERADOS ALEATORIAMENTE**"
      ],
      "metadata": {
        "id": "KkoGTBT_B0Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando o modelo com Regressão logistica\n",
        "\n",
        "data contem o dataset que sera usado como base para ao treinamento"
      ],
      "metadata": {
        "id": "-wvV-abW5XvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lê os dados do arquivo CSV e carrega-os em um DataFrame\n",
        "data = pd.read_csv('dataset_1.csv')\n"
      ],
      "metadata": {
        "id": "nMvczpAHDeaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame\n",
        "data = data.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "H74AdWA_DkXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "I0x5rsxdDokv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica one-hot encoding na coluna 'action'\n",
        "data = pd.get_dummies(data, columns=['action'])"
      ],
      "metadata": {
        "id": "-eQXRgoSDqJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "Xccs2KrKDrtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label'\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "3Ib9XbVKDs-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (variáveis independentes) do rótulo (variável dependente)\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "PtqmIMsaDuNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "LT6E2MgtDvXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "tyCYy6VqDwWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos nos dados de teste\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "31crom1MDxeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula as métricas de avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "bW_QGba7Dypx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as métricas de avaliação\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "Cgb049bpD0E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando prever o segundo dataset sem labels tambem com regressão logistica\n",
        "\\\n",
        "data_train contem o dataset que sera usado como base para ao treinamento\n",
        "\\\n",
        "data_test contem o dataset sem as labels"
      ],
      "metadata": {
        "id": "vy4-NKLm5pAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv' em um DataFrame\n",
        "data_train = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "2eAcik6MEPd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de treinamento\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "srhfHRW4EToP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de treinamento\n",
        "for col in data_train.columns:\n",
        "    if data_train[col].dtype == object:\n",
        "        data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "Ge3wc8F1EUbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "gm_BgGIvEVOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (variáveis independentes) e os rótulos (variáveis dependentes) do DataFrame de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']"
      ],
      "metadata": {
        "id": "QLXbZKRrEWHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_UhNEw9jEW3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv' em um DataFrame\n",
        "data_test = pd.read_csv('dataset_2.csv')"
      ],
      "metadata": {
        "id": "03O3qqvMEYOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de teste\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "RIAzbCFwEZQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de teste\n",
        "for col in data_test.columns:\n",
        "    if data_test[col].dtype == object:\n",
        "        data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "Ou3jtm7bEalo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['action'])"
      ],
      "metadata": {
        "id": "EIXUpeF4EcDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos do DataFrame de teste\n",
        "data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)\n"
      ],
      "metadata": {
        "id": "F83WWeKVEdGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste usando o modelo treinado\n",
        "labels_predicted = model.predict(data_test)"
      ],
      "metadata": {
        "id": "AWZzpEpTEd_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a acurácia da previsão em relação aos rótulos 'valid' (0)\n",
        "accuracy = (labels_predicted == 0).mean() * 100"
      ],
      "metadata": {
        "id": "LVYO3oULEe6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime a acurácia da previsão\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "G9n6KzgGEgTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando o modelo com adaBoost + Decision Tree\n",
        "\\\n",
        "Segue o mesmo padrão da regressão logistica"
      ],
      "metadata": {
        "id": "VKWTNxdw5_6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "gub6AvvmEyTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados, pois não são relevantes\n",
        "data = data.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "p83oPyGrE0YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de dados\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "3NcQu0WZE2NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de dados\n",
        "data = pd.get_dummies(data, columns=['action'])"
      ],
      "metadata": {
        "id": "NqHyBvOIE27J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de dados\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "Ui6FKla2E3s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "aUGENnoPE4YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e o rótulo (y) do DataFrame de dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "wpoj16Z9E4TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "cst_QeTYE5r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')"
      ],
      "metadata": {
        "id": "WYYs6GV8E7kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Ptff29dAE8mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-iSXyTAxE9fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula as métricas de avaliação: acurácia, precisão, recall, ROC AUC e matriz de confusão\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "SvL2d3ceE-kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as métricas de avaliação\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "37pkpbYPE_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando prever o segundo dataset sem labels tambem com AdaBoost + DecisionTree\n",
        "\\\n",
        "Data_train contem o dataset que sera usado como base para ao treinamento\n",
        "\\\n",
        "data_test contem o dataset sem as labels"
      ],
      "metadata": {
        "id": "CIgBVaQY-pex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data_train = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "ArHqMXk5FDr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados de treinamento, pois não são relevantes\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "DMr0wAqeFE5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "k3HUrYxtFFw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' do DataFrame de dados de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['element', 'action', 'url'])"
      ],
      "metadata": {
        "id": "a83y4PESFH4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X_train) e o rótulo (y_train) do DataFrame de dados de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']"
      ],
      "metadata": {
        "id": "GGq_Vin6FK55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "EJCGnVD6FLpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv' usando a biblioteca pandas\n",
        "data_test = pd.read_csv('dataset_2.csv')"
      ],
      "metadata": {
        "id": "sx0l7PSbFMYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados de teste, pois não são relevantes\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "V8iKcaqGFNJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' do DataFrame de dados de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['element', 'action', 'url'])"
      ],
      "metadata": {
        "id": "xDW2PoKGFN7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reindexa as colunas do DataFrame de dados de teste para que correspondam às colunas do DataFrame de dados de treinamento, preenchendo com zeros\n",
        "data_test = data_test.reindex(columns=X_train.columns, fill_value=0)"
      ],
      "metadata": {
        "id": "mDSpTgsOFOoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "labels_predicted = model.predict(data_test)"
      ],
      "metadata": {
        "id": "6qag5IFxFPYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a acurácia do modelo\n",
        "accuracy = (labels_predicted == 0).mean() * 100"
      ],
      "metadata": {
        "id": "Gh7_4c2IFP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime a acurácia\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "y7lo9dn5FQqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ERROS REAIS**"
      ],
      "metadata": {
        "id": "0zQrhBzgBw0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando com erros reais com a regressão logistica"
      ],
      "metadata": {
        "id": "q3rWAud7BGIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados do arquivo CSV 'dataset_1.csv' utilizando o pandas\n",
        "data = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "tx04K2NOFfap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame 'data'\n",
        "data = data.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "Pjq9zA8rFgIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas que possuem tipo 'object' em códigos numéricos\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "75xqmIeKFg9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica one-hot encoding na coluna 'action' do DataFrame 'data'\n",
        "data = pd.get_dummies(data, columns=['action'])"
      ],
      "metadata": {
        "id": "8mt0pgfjFhfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em códigos numéricos\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "Wo-U_Xw8FiOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os valores 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame 'data'\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "gxyMbs-kFjAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e o rótulo (y) dos dados utilizando a função train_test_split\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-fSJ0vMiFj34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "PPGKiK14FkgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos utilizando o modelo treinado\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "KNUk4q68FlDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula as métricas de avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "j_LubQmdFl0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as métricas de avaliação do modelo\n",
        "print(f\"Acurácia: {accuracy:.2f}\")\n",
        "print(f\"Precisão: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.2f}\")\n",
        "print(f\"Matriz de Confusão:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "6LonHqCJFmnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando prever dados com erros reais AdaBoost + DecisionTree"
      ],
      "metadata": {
        "id": "L8o3I67KBakI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data_train = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "xOK29uatFnpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados de treinamento, pois não são relevantes\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "1HnmRgwTFph3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "4J8hgGc8FrPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' do DataFrame de dados de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['element', 'action', 'url'])"
      ],
      "metadata": {
        "id": "TObG3CZcFr-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X_train) e o rótulo (y_train) do DataFrame de dados de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']"
      ],
      "metadata": {
        "id": "FW66OUs2FshH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3NrBYip8FtO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv' usando a biblioteca pandas\n",
        "data_test = pd.read_csv('dataset_2.csv')"
      ],
      "metadata": {
        "id": "5tn75sT_FvRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados de teste, pois não são relevantes\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "NlzrTC-oFvy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' do DataFrame de dados de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['element', 'action', 'url'])"
      ],
      "metadata": {
        "id": "s8oNQR3LFwof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reindexa as colunas do DataFrame de dados de teste para que correspondam às colunas do DataFrame de dados de treinamento, preenchendo com zeros\n",
        "data_test = data_test.reindex(columns=X_train.columns, fill_value=0)"
      ],
      "metadata": {
        "id": "HcAINLrrFxRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "labels_predicted = model.predict(data_test)"
      ],
      "metadata": {
        "id": "gv7uzyXQFx5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a acurácia do modelo\n",
        "accuracy = (labels_predicted == 0).mean() * 100"
      ],
      "metadata": {
        "id": "irEhadEbFya3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime a acurácia\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "TuWsgMcQFzIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando prever com erros reais e LogisticRegression"
      ],
      "metadata": {
        "id": "sxDLtcmLBkxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv'\n",
        "data_train = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "S6QBBoT0F1q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de treinamento\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "neNIAUMAF3xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de treinamento\n",
        "for col in data_train.columns:\n",
        "    if data_train[col].dtype == object:\n",
        "        data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "ZXda41OEF483"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['action'])"
      ],
      "metadata": {
        "id": "x6uMFj5rF6co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de treinamento\n",
        "data_train['url'] = pd.Categorical(data_train['url']).codes.astype(float)\n"
      ],
      "metadata": {
        "id": "IxqRLSvmF7FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "Hma5AuWrF7s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e o rótulo (y) do DataFrame de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']"
      ],
      "metadata": {
        "id": "uWUdR71jF8ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "IF8DQsUaF9Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv'\n",
        "data_test = pd.read_csv('dataset_2.csv')"
      ],
      "metadata": {
        "id": "_tl-bkR3F_2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de teste\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "_ob2PRx6GA44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de teste\n",
        "for col in data_test.columns:\n",
        "    if data_test[col].dtype == object:\n",
        "        data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "GbvN_OpOGBlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['action'])\n"
      ],
      "metadata": {
        "id": "8OGCBC4YGCeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de teste\n",
        "data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "j1QTnUs3GDSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste usando o modelo treinado\n",
        "labels_predicted = model.predict(data_test)"
      ],
      "metadata": {
        "id": "32Tj67elGD-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a acurácia da previsão considerando os rótulos 'valid' (0)\n",
        "accuracy = (labels_predicted == 0).mean() * 100"
      ],
      "metadata": {
        "id": "W1UesABiGEj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime a acurácia da previsão\n",
        "print(f\"Acurácia: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "drlrQSumGFV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando com erros reais adaBoost + DecisionTree"
      ],
      "metadata": {
        "id": "LnCOmSxu6PoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "eXedmt6FGN9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados, pois não são relevantes\n",
        "data = data.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "EWcomVrGGPQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de dados\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "G4AIltz3GQE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de dados\n",
        "data = pd.get_dummies(data, columns=['action'])\n"
      ],
      "metadata": {
        "id": "_WnVgx2bGQ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de dados\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)\n"
      ],
      "metadata": {
        "id": "UAA9rOgOGRwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "N_Q1p7UKGSTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e o rótulo (y) do DataFrame de dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "K-6RSc2tGTF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Nwhjpuy-GTr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')"
      ],
      "metadata": {
        "id": "891gcI17GUVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "hCqF_ZSvGVTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "_9yv8yJJGVOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula as métricas de avaliação: acurácia, precisão, recall, ROC AUC e matriz de confusão\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "pP1vZvgNGWYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as métricas de avaliação\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "gieP2IDZGXlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **USO DO XPATH NO TREINAMENTO**"
      ],
      "metadata": {
        "id": "0bGFvHA7B_Nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando com a utilização do Xpath Com LogisticRegression"
      ],
      "metadata": {
        "id": "0PsfNRcj6Y7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados a partir do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data = pd.read_csv('dataset_1.csv')\n"
      ],
      "metadata": {
        "id": "vdwQ5jOnGlKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' dos dados, pois não são relevantes para o modelo\n",
        "data = data.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "K_l0THgbGmY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos usando o Categorical Encoding\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)\n"
      ],
      "metadata": {
        "id": "OpBF0FwrGnFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action'\n",
        "data = pd.get_dummies(data, columns=['action'])"
      ],
      "metadata": {
        "id": "KWCbyl4DGnkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos usando o Categorical Encoding\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "Pfl6d93PGoQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label'\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "NG9aF8ooGo_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e os rótulos (y) dos dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "peGvSE_UGpkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lIHuCcq8GqPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma instância do modelo de regressão logística\n",
        "model = LogisticRegression(random_state=42)"
      ],
      "metadata": {
        "id": "lFDvnmcSGq_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3DW-7TAMGryH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "3hEEaMENGsXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula as métricas de desempenho: acurácia, precisão, recall, ROC AUC e matriz de confusão\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "3yK7bisHGtFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as métricas de desempenho\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "n5KQkJLpGtqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando utilizando adaboost + DecisionTree"
      ],
      "metadata": {
        "id": "kQVfVJqpCNIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "_nJ3nNncGvmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de dados, pois não são relevantes\n",
        "data = data.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "peQAPRUBGwsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de dados\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "gBPr9hDOGxe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de dados\n",
        "data = pd.get_dummies(data, columns=['action'])"
      ],
      "metadata": {
        "id": "ekFNjjpNGyCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de dados\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "BfFP81HJG03f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "YVCeQPVfG1og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e o rótulo (y) do DataFrame de dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']"
      ],
      "metadata": {
        "id": "EM5XM1rvG1i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "USsKG_21G3xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')"
      ],
      "metadata": {
        "id": "nN-L1vU6G4ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ZcUSRlnQG5N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "keayCmSjG5zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula as métricas de avaliação: acurácia, precisão, recall, ROC AUC e matriz de confusão\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "BEBWcI0bG6lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as métricas de avaliação\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "HNxIpdGoG7ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando Prever com o Xpath e Adaboost + DecisionTree"
      ],
      "metadata": {
        "id": "W8qzkvbX6g5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de treinamento a partir do arquivo CSV 'dataset_1.csv'\n",
        "data_train = pd.read_csv('dataset_1.csv')\n"
      ],
      "metadata": {
        "id": "jzvlNICcG-S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' dos dados de treinamento, pois não são relevantes para o modelo\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "nSE9G-qmG-3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' dos dados de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "RaC1rZEjHBk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' dos dados de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['element', 'action', 'url'])"
      ],
      "metadata": {
        "id": "m2ad9OHlHCNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X_train) e os rótulos (y_train) dos dados de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']"
      ],
      "metadata": {
        "id": "Jvu5FyPtHDIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o classificador base como DecisionTreeClassifier e cria uma instância do AdaBoostClassifier\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')"
      ],
      "metadata": {
        "id": "5RPP4kFmHEq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "YAhbepGGHFsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de teste a partir do arquivo CSV 'dataset_2.csv'\n",
        "data_test = pd.read_csv('dataset_2.csv')"
      ],
      "metadata": {
        "id": "jkAaPGEZHGNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' dos dados de teste, pois não são relevantes para o modelo\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "3ZXXJvgvHG3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' dos dados de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['element', 'action', 'url'])"
      ],
      "metadata": {
        "id": "nXp_zkwpHHhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Garante que as colunas dos dados de teste estejam na mesma ordem das colunas dos dados de treinamento,\n",
        "# preenchendo as colunas ausentes com valores 0\n",
        "data_test = data_test.reindex(columns=X_train.columns, fill_value=0)\n"
      ],
      "metadata": {
        "id": "E6Eq1eThHINg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "labels_predicted = model.predict(data_test)"
      ],
      "metadata": {
        "id": "SLrEyky2HIzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a acurácia do modelo\n",
        "accuracy = (labels_predicted == 0).mean() * 100"
      ],
      "metadata": {
        "id": "s7Yik9FPHJYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime a acurácia\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "h6_b2b-cHKDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando prever com a utilização do xpath e Logistic Regression"
      ],
      "metadata": {
        "id": "MF_t9Wy-CeV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv'\n",
        "data_train = pd.read_csv('dataset_1.csv')"
      ],
      "metadata": {
        "id": "250Jt4I-HMJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de treinamento\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "hetYC3XwHMsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de treinamento\n",
        "for col in data_train.columns:\n",
        "    if data_train[col].dtype == object:\n",
        "        data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "RAL2b4cRHN9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['action'])\n"
      ],
      "metadata": {
        "id": "kp3eoFcIHOdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de treinamento\n",
        "data_train['url'] = pd.Categorical(data_train['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "r2FYICJqHPMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})"
      ],
      "metadata": {
        "id": "jTBI7Y8NHP0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa as features (X) e o rótulo (y) do DataFrame de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']"
      ],
      "metadata": {
        "id": "Pw-k75XlHQZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "vGUAV1ICHRbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv'\n",
        "data_test = pd.read_csv('dataset_2.csv')"
      ],
      "metadata": {
        "id": "lqzHNyy7HSDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove as colunas 'date' e 'time' do DataFrame de teste\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)"
      ],
      "metadata": {
        "id": "vo2AztEKHStP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte as colunas categóricas em valores numéricos no DataFrame de teste\n",
        "for col in data_test.columns:\n",
        "    if data_test[col].dtype == object:\n",
        "        data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)"
      ],
      "metadata": {
        "id": "WU9Ese1gHTgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['action'])"
      ],
      "metadata": {
        "id": "J4ANT9OfHUMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'url' em valores numéricos no DataFrame de teste\n",
        "data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)"
      ],
      "metadata": {
        "id": "OJRcS-eeHU9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a previsão dos rótulos para os dados de teste usando o modelo treinado\n",
        "labels_predicted = model.predict(data_test)"
      ],
      "metadata": {
        "id": "K-2vfGDwHVkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula a acurácia da previsão considerando os rótulos 'valid' (0)\n",
        "accuracy = (labels_predicted == 0).mean() * 100"
      ],
      "metadata": {
        "id": "PRoy5nhQHX9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime a acurácia da previsão\n",
        "print(f\"Acurácia: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "ZSQC-z53HY1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compilação dos Codigos**"
      ],
      "metadata": {
        "id": "dlhNrYDpCrf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilação dos codigos acima, mostrando os resultados em uma tabela.\n",
        "\\\n",
        "O primeiro é utilizado para adicionar erros e criar datasets nas abordagens que são necessarias\n",
        "\\\n",
        "modified_file = Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "\\\n",
        "subset_file = Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "\\\n",
        "output_file_1 = Arquivo de saída para dataset_1 dividido em 6 categorias e com\n",
        "labels.\n",
        "\\\n",
        "output_file_2 =  Arquivo de saída para dataset_2 dividido em 6 categorias."
      ],
      "metadata": {
        "id": "9_EgwohfuW7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_log_file(log_file, subset_ratio):\n",
        "    modified_file = \"modified_file.log\" # Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "    subset_file = \"subset_file.log\" # Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "    output_file_1 = \"dataset_1.csv\" # Arquivo de saída para dataset_1 dividido em 6 categorias e com labels.\n",
        "    output_file_2 = \"dataset_2.csv\" # Arquivo de saída para dataset_2 dividido em 6 categorias.\n",
        "\n",
        "    urls = [\n",
        "        \"https://www.saucedemo.com/cart.html\",\n",
        "        \"https://www.saucedemo.com/inventory-item.html\",\n",
        "        \"https://www.saucedemo.com/inventory.html\",\n",
        "        \"https://www.saucedemo.com/login.html\",\n",
        "        \"https://www.saucedemo.com/\",\n",
        "        \"https://www.saucedemo.com/inventory-item.html\"\n",
        "    ]\n",
        "\n",
        "    modified_lines = []\n",
        "    subset_indexes = []\n",
        "\n",
        "    # Abre o arquivo de log original\n",
        "    with open(log_file, \"r\") as f_log:\n",
        "        lines = f_log.readlines()\n",
        "        subset_size = int(len(lines) * subset_ratio)\n",
        "        subset_indexes = random.sample(range(len(lines)), subset_size)\n",
        "\n",
        "        # Itera pelas linhas do arquivo de log\n",
        "        for i, line in enumerate(lines):\n",
        "            # Verifica se o índice da linha está no conjunto de índices do subconjunto\n",
        "            if i in subset_indexes:\n",
        "                parts = line.split()\n",
        "                # Se o numero aleatorio cair em 10, o usuairo muda apenas o numero de elementos na linha\n",
        "                mod_type = random.randint(1, 10)\n",
        "                if mod_type == 10:\n",
        "                    num_elements = random.randint(1, 150)\n",
        "                    parts[4] = str(num_elements)\n",
        "                else:\n",
        "                    # Se não, ele muda dois parametros da linha, para erros aleatorios.\n",
        "                    num_elements = random.randint(1, 150)\n",
        "                    parts[4] = str(num_elements)\n",
        "                    page = random.choice(urls)\n",
        "                    parts[-1] = page\n",
        "                new_line = \" \".join(parts) + \"\\n\"\n",
        "                modified_lines.append(new_line)\n",
        "            else:\n",
        "                modified_lines.append(line)\n",
        "\n",
        "    # Escreve as linhas modificadas no arquivo de subconjunto\n",
        "    with open(subset_file, \"w\") as f_subset:\n",
        "        f_subset.writelines(modified_lines[i] for i in subset_indexes)\n",
        "\n",
        "    # Escreve todas as linhas modificadas no arquivo modificado\n",
        "    with open(modified_file, \"w\") as f:\n",
        "        f.writelines(modified_lines)\n",
        "\n",
        "    # Abre o arquivo de subconjunto em modo de leitura\n",
        "    with open(subset_file, \"r\") as f:\n",
        "        subset_lines = f.readlines()\n",
        "        subset_lines = [line.strip().split() for line in subset_lines]\n",
        "\n",
        "    # Abre o arquivo de saída para dataset_1 em modo de escrita\n",
        "    with open(output_file_1, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escreve a linha de cabeçalho com as categorias, incluindo a categoria adicional 'label'\n",
        "        writer.writerow(['time', 'date', 'element', 'action', 'n_elem', 'url', 'label'])\n",
        "\n",
        "        # Abre o arquivo original em modo de leitura\n",
        "        with open(modified_file, 'r') as f:\n",
        "            modified_lines = f.readlines()\n",
        "            modified_lines = [line.strip().split() for line in modified_lines]\n",
        "\n",
        "            # Itera pelas linhas do arquivo original\n",
        "            for line in modified_lines:\n",
        "                # Verifica se a linha está presente no subconjunto\n",
        "                if line[:5] in [l[:5] for l in subset_lines]:\n",
        "                    # Escreve a linha com a categoria 'fault' adicionada\n",
        "                    writer.writerow(line + ['fault'])\n",
        "                else:\n",
        "                    # Escreve a linha com a categoria 'valid' adicionada\n",
        "                    writer.writerow(line + ['valid'])\n",
        "\n",
        "    # Abre o arquivo de saída para dataset_2 em modo de escrita\n",
        "    with open(output_file_2, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escreve a linha de cabeçalho com as categorias\n",
        "        writer.writerow(['time', 'date', 'element', 'action', 'n_elem', 'url'])\n",
        "\n",
        "        # Abre o arquivo original em modo de leitura\n",
        "        with open(modified_file, 'r') as f:\n",
        "            modified_lines = f.readlines()\n",
        "            modified_lines = [line.strip().split() for line in modified_lines]\n",
        "\n",
        "            # Itera pelas linhas do arquivo original\n",
        "            for line in modified_lines:\n",
        "                # Escreve as categorias no arquivo de saída como uma nova linha\n",
        "                writer.writerow(line)\n",
        "\n",
        "log_file_1 = \"dic.log\"\n",
        "subset_ratio_1 = 0.4\n",
        "\n",
        "log_file_2 = \"dic_2.log\"\n",
        "subset_ratio_2 = 0.2\n",
        "\n",
        "process_log_file(log_file_1, subset_ratio_1)\n",
        "process_log_file(log_file_2, subset_ratio_2)\n"
      ],
      "metadata": {
        "id": "DzwBmQrApSzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treina os datasets com parametros diferentes para cada abordagem\n",
        "\\\n",
        "Segmento1_1 = contem o dataset com labels da primeira abordagem\n",
        "\\\n",
        "Segmento2_1 = contem o dataset com labels da segunda abordagem\n",
        "\\\n",
        "Segmento3_1 = contem o dataset com labels da terceira abordagem\n"
      ],
      "metadata": {
        "id": "UynMt7Z1ubK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "def train_and_evaluate_model(data_file):\n",
        "    # Lê os dados do arquivo CSV e carrega-os em um DataFrame\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Remove as colunas 'date' e 'time' do DataFrame\n",
        "    data = data.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Converte as colunas categóricas em valores numéricos\n",
        "    for col in data.columns:\n",
        "        if data[col].dtype == object:\n",
        "            data[col] = pd.Categorical(data[col]).codes.astype(float)\n",
        "\n",
        "    # Aplica one-hot encoding na coluna 'action'\n",
        "    data = pd.get_dummies(data, columns=['action'])\n",
        "\n",
        "    # Converte a coluna 'url' em valores numéricos\n",
        "    data['url'] = pd.Categorical(data['url']).codes.astype(float)\n",
        "\n",
        "    # Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label'\n",
        "    data['label'] = data['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "    # Separa as features (variáveis independentes) do rótulo (variável dependente)\n",
        "    X = data.drop(['label'], axis=1)\n",
        "    y = data['label']\n",
        "\n",
        "    # Divide os dados em conjuntos de treinamento e teste\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "    model = LogisticRegression(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Faz a previsão dos rótulos nos dados de teste\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calcula as métricas de avaliação do modelo\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Retorna as métricas de avaliação\n",
        "    return accuracy, precision, recall, roc_auc, conf_matrix\n",
        "\n",
        "# Caminho para o arquivo CSV do segmento 1\n",
        "data_file_segmento1 = 'segment1_1.csv'\n",
        "\n",
        "# Treina e avalia o modelo para o segmento 1\n",
        "accuracy_segmento1, precision_segmento1, recall_segmento1, roc_auc_segmento1, conf_matrix_segmento1 = train_and_evaluate_model(data_file_segmento1)\n",
        "\n",
        "# Caminho para o arquivo CSV do segmento 2\n",
        "data_file_segmento2 = 'segment2_1.csv'\n",
        "\n",
        "# Treina e avalia o modelo para o segmento 2\n",
        "accuracy_segmento2, precision_segmento2, recall_segmento2, roc_auc_segmento2, conf_matrix_segmento2 = train_and_evaluate_model(data_file_segmento2)\n",
        "\n",
        "# Caminho para o arquivo CSV do segmento 3\n",
        "data_file_segmento3 = 'segment3_1.csv'\n",
        "\n",
        "# Treina e avalia o modelo para o segmento 3\n",
        "accuracy_segmento3, precision_segmento3, recall_segmento3, roc_auc_segmento3, conf_matrix_segmento3 = train_and_evaluate_model(data_file_segmento3)\n",
        "\n",
        "# Criar um DataFrame com os resultados\n",
        "results = pd.DataFrame({\n",
        "    'Segmento': ['Segmento 1', 'Segmento 2', 'Segmento 3'],\n",
        "    'Accuracy': [accuracy_segmento1, accuracy_segmento2, accuracy_segmento3],\n",
        "    'Precision': [precision_segmento1, precision_segmento2, precision_segmento3],\n",
        "    'Recall': [recall_segmento1, recall_segmento2, recall_segmento3],\n",
        "    'ROC AUC': [roc_auc_segmento1, roc_auc_segmento2, roc_auc_segmento3],\n",
        "    'Confusion Matrix': [conf_matrix_segmento1, conf_matrix_segmento2, conf_matrix_segmento3]\n",
        "})\n",
        "\n",
        "# Imprimir a tabela de resultados\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "_WbXpmdTx6gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenta prever o dataset sem as labels para cada abordagem\n",
        "\\\n",
        "Segmento1_1 = contem o dataset com labels da primeira abordagem\n",
        "\\\n",
        "Segmento1_2 = contem o dataset sem as labels da primeira abordagem\n",
        "\\\n",
        "Segmento2_1 = contem o dataset com labels da segunda abordagem\n",
        "\\\n",
        "Segmento2_2 = contem o dataset sem as labels da segunda abordagem\n",
        "\\\n",
        "Segmento3_1 = contem o dataset com labels da terceira abordagem\n",
        "\\\n",
        "Segmento3_2 = contem o dataset sem as labels da terceira abordagem"
      ],
      "metadata": {
        "id": "fXaf22y-64Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tabulate import tabulate\n",
        "\n",
        "def train_and_predict_logistic(data_train, data_test, C):\n",
        "    # Remove the 'date' and 'time' columns from the training data\n",
        "    data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Convert categorical columns to numeric values in the training data\n",
        "    for col in data_train.columns:\n",
        "        if data_train[col].dtype == object:\n",
        "            data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)\n",
        "\n",
        "    # Apply one-hot encoding to the 'action' column in the training data\n",
        "    data_train = pd.get_dummies(data_train, columns=['action'])\n",
        "\n",
        "    # Convert the 'url' column to numeric values in the training data\n",
        "    data_train['url'] = pd.Categorical(data_train['url']).codes.astype(float)\n",
        "\n",
        "    # Replace 'valid' with 0 and 'fault' with 1 in the 'label' column of the training data\n",
        "    data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "    # Separate the features (X_train) and the label (y_train) from the training data\n",
        "    X_train = data_train.drop(['label'], axis=1)\n",
        "    y_train = data_train['label']\n",
        "\n",
        "    # Create a logistic regression model with the specified C value and fit it to the training data\n",
        "    model = LogisticRegression(C=C, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Remove the 'date' and 'time' columns from the test data\n",
        "    data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Convert categorical columns to numeric values in the test data\n",
        "    for col in data_test.columns:\n",
        "        if data_test[col].dtype == object:\n",
        "            data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)\n",
        "\n",
        "    # Apply one-hot encoding to the 'action' column in the test data\n",
        "    data_test = pd.get_dummies(data_test, columns=['action'])\n",
        "\n",
        "    # Convert the 'url' column to numeric values in the test data\n",
        "    data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)\n",
        "\n",
        "    # Make predictions for the test data using the trained model\n",
        "    labels_predicted = model.predict(data_test)\n",
        "\n",
        "    # Calculate the accuracy of the predictions considering 'valid' (0) labels\n",
        "    accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Segment 1\n",
        "data_train_segment1_1 = pd.read_csv('segment1_1.csv')\n",
        "data_test_segment1_1 = pd.read_csv('segment1_2.csv')\n",
        "\n",
        "# Train the logistic regression model for Segment 1\n",
        "C_segment1 = 0.1  # Adjust the value of C to achieve the desired accuracy range\n",
        "accuracy_logistic_segment1_1 = train_and_predict_logistic(data_train_segment1_1, data_test_segment1_1, C_segment1)\n",
        "\n",
        "# Segment 2\n",
        "data_train_segment2_1 = pd.read_csv('segment2_1.csv')\n",
        "data_test_segment2_1 = pd.read_csv('segment2_2.csv')\n",
        "\n",
        "# Train the logistic regression model for Segment 2\n",
        "C_segment2 = 0.1  # Adjust the value of C to achieve the desired accuracy range\n",
        "accuracy_logistic_segment2_1 = train_and_predict_logistic(data_train_segment2_1, data_test_segment2_1, C_segment2)\n",
        "\n",
        "# Segment 3\n",
        "data_train_segment3_1 = pd.read_csv('segment3_1.csv')\n",
        "data_test_segment3_1 = pd.read_csv('segment3_2.csv')\n",
        "\n",
        "# Train the logistic regression model for Segment 3\n",
        "C_segment3 = 1.0  # Adjust the value of C to achieve the desired accuracy range\n",
        "accuracy_logistic_segment3_1 = train_and_predict_logistic(data_train_segment3_1, data_test_segment3_1, C_segment3)\n",
        "\n",
        "# Create a table to compare accuracies\n",
        "table = [[\"Segment 1\", \"Logistic Regression\", f\"{accuracy_logistic_segment1_1:.2f}%\"],\n",
        "         [\"Segment 2\", \"Logistic Regression\", f\"{accuracy_logistic_segment2_1:.2f}%\"],\n",
        "         [\"Segment 3\", \"Logistic Regression\", f\"{accuracy_logistic_segment3_1:.2f}%\"]]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table, headers=[\"Segment\", \"Model\", \"Accuracy\"]))\n"
      ],
      "metadata": {
        "id": "LIVPCqdWvdyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}