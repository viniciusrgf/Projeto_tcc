{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkKLhtSGyZvTcG4kQVpiOV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrgf/Projeto_tcc/blob/main/Modelo_tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento Modelo #1 Tcc: Criação de modelo capaz de analisar e desenvolver testes simples Web\n",
        "\n",
        "Etapa #1: Requerimentos do Modelo - Modelo deve analisar uma pagina web e identificar elementos comuns, como caixas de texto e botões e desenvolver testes para esses elementos\n",
        "\n",
        "Etapa #2: Coletar e rotular dados atraves de uma coleta dados de treinamento que possam ser usados ​​para treinar o modelo. Esses dados podem incluir screenshots de páginas da web, informações sobre os elementos da página e testes de exemplo que já foram escritos.\n",
        "\n",
        "Etapa #3: Treinar esse modelo para poder conseguir prever se um novo 'teste' sem uma label é valido ou não\n",
        "\n",
        "Site Exemplo: http://saucedemo.com/ https://testpages.herokuapp.com/styled/index.html https://theuselessweb.com/"
      ],
      "metadata": {
        "id": "-TRD6egfjSZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "ZTetLn18jWOB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYf3_9ffjMsj"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import random\n",
        "import csv\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As abordagens serão apresentadas, inicialmente, como foram feitas passo a passo, apos isso existe um codigo unico que fara, as analises e printara uma tabela com os resultados."
      ],
      "metadata": {
        "id": "SjIiFEjw1L94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo para inserir porcentagem de erros em um arquivo de log, gerando dois tipos de arquivos, um modified que representam o arquivo com os erros inseridos e o subset que é o arquivo que contem os erros que foram gerados e inseridos no outro documento\n"
      ],
      "metadata": {
        "id": "Nj089Umz1bII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "log_file = Arquivo de log original.\n",
        "modified_file = Arquivo original contendo linhas alteradas.\n",
        "\\\n",
        "subset_file= = Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "\\\n",
        "subset_ratio = Variavel contendo a porcentagem de erros que serão adicionadas no arquivo.\n",
        "\\\n",
        "element_ratio = Porcentagem de chance de mudar apenas um dos parametros"
      ],
      "metadata": {
        "id": "4EqT5lrw2TGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "log_file = \"dic_mod.log\" # Arquivo original com os testes do website\n",
        "modified_file = \"modified_file.log\" # Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "subset_file = \"subset_file.log\" # Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "subset_ratio = 0.4 # porcentagem de erros que o usuario quer no arquivo\n",
        "element_ratio = 0.2 # Porcentagem de chance de mudar apenas um dos parametros\n",
        "\n",
        "urls = [\n",
        "    \"https://www.saucedemo.com/cart.html\",\n",
        "    \"https://www.saucedemo.com/inventory-item.html\",\n",
        "    \"https://www.saucedemo.com/inventory.html\",\n",
        "    \"https://www.saucedemo.com/login.html\",\n",
        "    \"https://www.saucedemo.com/\",\n",
        "    \"https://www.saucedemo.com/inventory-item.html\"\n",
        "]\n",
        "\n",
        "modified_lines = []\n",
        "subset_indexes = []\n",
        "\n",
        "# Abre o arquivo de log original\n",
        "with open(log_file, \"r\") as f_log:\n",
        "    lines = f_log.readlines()\n",
        "    subset_size = int(len(lines) * subset_ratio)\n",
        "    subset_indexes = random.sample(range(len(lines)), subset_size)\n",
        "\n",
        "    # Itera pelas linhas do arquivo de log\n",
        "    for i, line in enumerate(lines):\n",
        "        # Verifica se o índice da linha está no conjunto de índices do subconjunto\n",
        "        if i in subset_indexes:\n",
        "            parts = line.split()\n",
        "            # Se o numero aleatorio cair em 10, o usuairo muda apenas o numero de elementos na linha\n",
        "            mod_type = random.randint(1, 10)\n",
        "            if mod_type == 10:\n",
        "                num_elements = random.randint(1, 150)\n",
        "                parts[4] = str(num_elements)\n",
        "            else:\n",
        "              # Se não, ele muda dois parametros da linha, para erros aleatorios.\n",
        "                num_elements = random.randint(1, 150)\n",
        "                parts[4] = str(num_elements)\n",
        "                page = random.choice(urls)\n",
        "                parts[-1] = page\n",
        "            new_line = \" \".join(parts) + \"\\n\"\n",
        "            modified_lines.append(new_line)\n",
        "        else:\n",
        "            modified_lines.append(line)\n",
        "\n",
        "# Escreve as linhas modificadas no arquivo de subconjunto\n",
        "with open(subset_file, \"w\") as f_subset:\n",
        "    f_subset.writelines(modified_lines[i] for i in subset_indexes)\n",
        "\n",
        "# Escreve todas as linhas modificadas no arquivo modificado\n",
        "with open(modified_file, \"w\") as f:\n",
        "    f.writelines(modified_lines)\n"
      ],
      "metadata": {
        "id": "Vr9U9mdl1f9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo abaixo utilizado para criar o dataset sem as labels de valid e fault\n",
        "\n",
        "subset_file = Arquivo original contendo linhas alteradas e não alteradas.\n",
        "\\\n",
        "original_file = Arquivo contendo os erros que serão 'labeled' fault.\n",
        "\\\n",
        "output_file = Arquivo de saida em formato de dataset."
      ],
      "metadata": {
        "id": "bssOnxxo1jRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "subset_file = 'subset_file.log' # Arquivo com tanto as linhas alteradas como as linhas não alteradas.\n",
        "original_file = 'modified_file.log' # Arquivo contendo apenas as linhas que foram alteradas.\n",
        "output_file = 'dataset_2.csv' # Arquivo de saida em formato dataset dividido em 6 categorias.\n",
        "\n",
        "categories = ['time', 'date', 'element', 'action', 'n_elem', 'url']\n",
        "\n",
        "# Abre o arquivo de saída em modo de escrita\n",
        "with open(output_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # Escreve a linha de cabeçalho com as categorias\n",
        "    writer.writerow(categories)\n",
        "\n",
        "    # Abre o arquivo original em modo de leitura\n",
        "    with open(original_file, 'r') as f:\n",
        "        original_lines = f.readlines()\n",
        "        original_lines = [line.strip() for line in original_lines]\n",
        "\n",
        "        # Itera pelas linhas do arquivo original\n",
        "        for line in original_lines:\n",
        "            # Separa as categorias da linha\n",
        "            line_categories = line.split(' ')\n",
        "\n",
        "            # Completa com strings vazias se o número de categorias for inferior a 6\n",
        "            while len(line_categories) < 6:\n",
        "                line_categories.append('')\n",
        "\n",
        "            # Escreve as categorias no arquivo de saída como uma nova linha\n",
        "            writer.writerow(line_categories)\n"
      ],
      "metadata": {
        "id": "6DidyJqg1pDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando o modelo com Regressão logistica\n",
        "\n",
        "Data_train contem o dataset que sera usado como base para ao treinamento"
      ],
      "metadata": {
        "id": "-wvV-abW5XvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Lê os dados do arquivo CSV e carrega-os em um DataFrame\n",
        "data = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' do DataFrame\n",
        "data = data.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Converte as colunas categóricas em valores numéricos\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)\n",
        "\n",
        "# Aplica one-hot encoding na coluna 'action'\n",
        "data = pd.get_dummies(data, columns=['action'])\n",
        "\n",
        "# Converte a coluna 'url' em valores numéricos\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)\n",
        "\n",
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label'\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Separa as features (variáveis independentes) do rótulo (variável dependente)\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Divide os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faz a previsão dos rótulos nos dados de teste\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcula as métricas de avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Imprime as métricas de avaliação\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
      ],
      "metadata": {
        "id": "GSb2hNB05avY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando prever o segundo dataset sem labels tambem com regressão logistica\n",
        "\\\n",
        "Data_train contem o dataset que sera usado como base para ao treinamento\n",
        "\\\n",
        "data_test contem o dataset sem as labels"
      ],
      "metadata": {
        "id": "vy4-NKLm5pAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv' em um DataFrame\n",
        "data_train = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' do DataFrame de treinamento\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Converte as colunas categóricas em valores numéricos no DataFrame de treinamento\n",
        "for col in data_train.columns:\n",
        "    if data_train[col].dtype == object:\n",
        "        data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)\n",
        "\n",
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Separa as features (variáveis independentes) e os rótulos (variáveis dependentes) do DataFrame de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']\n",
        "\n",
        "# Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv' em um DataFrame\n",
        "data_test = pd.read_csv('dataset_2.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' do DataFrame de teste\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Converte as colunas categóricas em valores numéricos no DataFrame de teste\n",
        "for col in data_test.columns:\n",
        "    if data_test[col].dtype == object:\n",
        "        data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)\n",
        "\n",
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['action'])\n",
        "\n",
        "# Converte a coluna 'url' em valores numéricos do DataFrame de teste\n",
        "data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)\n",
        "\n",
        "# Faz a previsão dos rótulos para os dados de teste usando o modelo treinado\n",
        "labels_predicted = model.predict(data_test)\n",
        "\n",
        "# Calcula a acurácia da previsão em relação aos rótulos 'valid' (0)\n",
        "accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "# Imprime a acurácia da previsão\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Q5vxONpd5ymT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando o modelo com adaBoost + Decision Tree\n",
        "\\\n",
        "Segue o mesmo padrão da regressão logistica"
      ],
      "metadata": {
        "id": "VKWTNxdw5_6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Carrega os dados do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' do DataFrame de dados, pois não são relevantes\n",
        "data = data.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Converte as colunas categóricas em valores numéricos no DataFrame de dados\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)\n",
        "\n",
        "# Aplica o one-hot encoding na coluna 'action' do DataFrame de dados\n",
        "data = pd.get_dummies(data, columns=['action'])\n",
        "\n",
        "# Converte a coluna 'url' em valores numéricos no DataFrame de dados\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)\n",
        "\n",
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Separa as features (X) e o rótulo (y) do DataFrame de dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Divide os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')\n",
        "\n",
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcula as métricas de avaliação: acurácia, precisão, recall, ROC AUC e matriz de confusão\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Imprime as métricas de avaliação\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
      ],
      "metadata": {
        "id": "_Gxo_UxK6Gm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando com erros reais"
      ],
      "metadata": {
        "id": "LnCOmSxu6PoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carrega os dados de treinamento do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data_train = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' do DataFrame de dados de treinamento, pois não são relevantes\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' do DataFrame de dados de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' do DataFrame de dados de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['element', 'action', 'url'])\n",
        "\n",
        "# Separa as features (X_train) e o rótulo (y_train) do DataFrame de dados de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']\n",
        "\n",
        "# Define o estimador base como DecisionTreeClassifier e cria um modelo AdaBoost\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Carrega os dados de teste do arquivo CSV 'dataset_2.csv' usando a biblioteca pandas\n",
        "data_test = pd.read_csv('dataset_2.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' do DataFrame de dados de teste, pois não são relevantes\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' do DataFrame de dados de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['element', 'action', 'url'])\n",
        "\n",
        "# Reindexa as colunas do DataFrame de dados de teste para que correspondam às colunas do DataFrame de dados de treinamento, preenchendo com zeros\n",
        "data_test = data_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "labels_predicted = model.predict(data_test)\n",
        "\n",
        "# Calcula a acurácia do modelo\n",
        "accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "# Imprime a acurácia\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "yfzKK9pN6Uc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando com a utilização do Xpath"
      ],
      "metadata": {
        "id": "0PsfNRcj6Y7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Carrega os dados a partir do arquivo CSV 'dataset_1.csv' usando a biblioteca pandas\n",
        "data = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' dos dados, pois não são relevantes para o modelo\n",
        "data = data.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Converte as colunas categóricas em valores numéricos usando o Categorical Encoding\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        data[col] = pd.Categorical(data[col]).codes.astype(float)\n",
        "\n",
        "# Aplica o one-hot encoding na coluna 'action'\n",
        "data = pd.get_dummies(data, columns=['action'])\n",
        "\n",
        "# Converte a coluna 'url' em valores numéricos usando o Categorical Encoding\n",
        "data['url'] = pd.Categorical(data['url']).codes.astype(float)\n",
        "\n",
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label'\n",
        "data['label'] = data['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Separa as features (X) e os rótulos (y) dos dados\n",
        "X = data.drop(['label'], axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Divide os dados em conjuntos de treinamento e teste usando train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Cria uma instância do modelo de regressão logística\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcula as métricas de desempenho: acurácia, precisão, recall, ROC AUC e matriz de confusão\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Imprime as métricas de desempenho\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
      ],
      "metadata": {
        "id": "AHqprGZL6cVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentando Prever com o Xpath"
      ],
      "metadata": {
        "id": "W8qzkvbX6g5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carrega os dados de treinamento a partir do arquivo CSV 'dataset_1.csv'\n",
        "data_train = pd.read_csv('dataset_1.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' dos dados de treinamento, pois não são relevantes para o modelo\n",
        "data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label' dos dados de treinamento\n",
        "data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' dos dados de treinamento\n",
        "data_train = pd.get_dummies(data_train, columns=['element', 'action', 'url'])\n",
        "\n",
        "# Separa as features (X_train) e os rótulos (y_train) dos dados de treinamento\n",
        "X_train = data_train.drop(['label'], axis=1)\n",
        "y_train = data_train['label']\n",
        "\n",
        "# Define o classificador base como DecisionTreeClassifier e cria uma instância do AdaBoostClassifier\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "model = AdaBoostClassifier(base_estimator=base_estimator, algorithm='SAMME')\n",
        "\n",
        "# Treina o modelo usando os dados de treinamento\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Carrega os dados de teste a partir do arquivo CSV 'dataset_2.csv'\n",
        "data_test = pd.read_csv('dataset_2.csv')\n",
        "\n",
        "# Remove as colunas 'date' e 'time' dos dados de teste, pois não são relevantes para o modelo\n",
        "data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "# Aplica o one-hot encoding nas colunas 'element', 'action' e 'url' dos dados de teste\n",
        "data_test = pd.get_dummies(data_test, columns=['element', 'action', 'url'])\n",
        "\n",
        "# Garante que as colunas dos dados de teste estejam na mesma ordem das colunas dos dados de treinamento,\n",
        "# preenchendo as colunas ausentes com valores 0\n",
        "data_test = data_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "# Faz a previsão dos rótulos para os dados de teste\n",
        "labels_predicted = model.predict(data_test)\n",
        "\n",
        "# Calcula a acurácia do modelo\n",
        "accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "# Imprime a acurácia\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "zoA1a_Mo6k5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilação dos codigos acima, mostrando os resultados em uma tabela.\n",
        "\\\n",
        "O primeiro é utilizado para adicionar erros e criar datasets nas abordagens que são necessarias\n",
        "\\\n",
        "modified_file = Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "\\\n",
        "subset_file = Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "\\\n",
        "output_file_1 = Arquivo de saída para dataset_1 dividido em 6 categorias e com\n",
        "labels.\n",
        "\\\n",
        "output_file_2 =  Arquivo de saída para dataset_2 dividido em 6 categorias."
      ],
      "metadata": {
        "id": "9_EgwohfuW7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_log_file(log_file, subset_ratio):\n",
        "    modified_file = \"modified_file.log\" # Arquivo gerado contendo apenas as linhas que foram alteradas.\n",
        "    subset_file = \"subset_file.log\" # Arquivo que contem as linhas não alteradas e as linhas alteradas.\n",
        "    output_file_1 = \"dataset_1.csv\" # Arquivo de saída para dataset_1 dividido em 6 categorias e com labels.\n",
        "    output_file_2 = \"dataset_2.csv\" # Arquivo de saída para dataset_2 dividido em 6 categorias.\n",
        "\n",
        "    urls = [\n",
        "        \"https://www.saucedemo.com/cart.html\",\n",
        "        \"https://www.saucedemo.com/inventory-item.html\",\n",
        "        \"https://www.saucedemo.com/inventory.html\",\n",
        "        \"https://www.saucedemo.com/login.html\",\n",
        "        \"https://www.saucedemo.com/\",\n",
        "        \"https://www.saucedemo.com/inventory-item.html\"\n",
        "    ]\n",
        "\n",
        "    modified_lines = []\n",
        "    subset_indexes = []\n",
        "\n",
        "    # Abre o arquivo de log original\n",
        "    with open(log_file, \"r\") as f_log:\n",
        "        lines = f_log.readlines()\n",
        "        subset_size = int(len(lines) * subset_ratio)\n",
        "        subset_indexes = random.sample(range(len(lines)), subset_size)\n",
        "\n",
        "        # Itera pelas linhas do arquivo de log\n",
        "        for i, line in enumerate(lines):\n",
        "            # Verifica se o índice da linha está no conjunto de índices do subconjunto\n",
        "            if i in subset_indexes:\n",
        "                parts = line.split()\n",
        "                # Se o numero aleatorio cair em 10, o usuairo muda apenas o numero de elementos na linha\n",
        "                mod_type = random.randint(1, 10)\n",
        "                if mod_type == 10:\n",
        "                    num_elements = random.randint(1, 150)\n",
        "                    parts[4] = str(num_elements)\n",
        "                else:\n",
        "                    # Se não, ele muda dois parametros da linha, para erros aleatorios.\n",
        "                    num_elements = random.randint(1, 150)\n",
        "                    parts[4] = str(num_elements)\n",
        "                    page = random.choice(urls)\n",
        "                    parts[-1] = page\n",
        "                new_line = \" \".join(parts) + \"\\n\"\n",
        "                modified_lines.append(new_line)\n",
        "            else:\n",
        "                modified_lines.append(line)\n",
        "\n",
        "    # Escreve as linhas modificadas no arquivo de subconjunto\n",
        "    with open(subset_file, \"w\") as f_subset:\n",
        "        f_subset.writelines(modified_lines[i] for i in subset_indexes)\n",
        "\n",
        "    # Escreve todas as linhas modificadas no arquivo modificado\n",
        "    with open(modified_file, \"w\") as f:\n",
        "        f.writelines(modified_lines)\n",
        "\n",
        "    # Abre o arquivo de subconjunto em modo de leitura\n",
        "    with open(subset_file, \"r\") as f:\n",
        "        subset_lines = f.readlines()\n",
        "        subset_lines = [line.strip().split() for line in subset_lines]\n",
        "\n",
        "    # Abre o arquivo de saída para dataset_1 em modo de escrita\n",
        "    with open(output_file_1, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escreve a linha de cabeçalho com as categorias, incluindo a categoria adicional 'label'\n",
        "        writer.writerow(['time', 'date', 'element', 'action', 'n_elem', 'url', 'label'])\n",
        "\n",
        "        # Abre o arquivo original em modo de leitura\n",
        "        with open(modified_file, 'r') as f:\n",
        "            modified_lines = f.readlines()\n",
        "            modified_lines = [line.strip().split() for line in modified_lines]\n",
        "\n",
        "            # Itera pelas linhas do arquivo original\n",
        "            for line in modified_lines:\n",
        "                # Verifica se a linha está presente no subconjunto\n",
        "                if line[:5] in [l[:5] for l in subset_lines]:\n",
        "                    # Escreve a linha com a categoria 'fault' adicionada\n",
        "                    writer.writerow(line + ['fault'])\n",
        "                else:\n",
        "                    # Escreve a linha com a categoria 'valid' adicionada\n",
        "                    writer.writerow(line + ['valid'])\n",
        "\n",
        "    # Abre o arquivo de saída para dataset_2 em modo de escrita\n",
        "    with open(output_file_2, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # Escreve a linha de cabeçalho com as categorias\n",
        "        writer.writerow(['time', 'date', 'element', 'action', 'n_elem', 'url'])\n",
        "\n",
        "        # Abre o arquivo original em modo de leitura\n",
        "        with open(modified_file, 'r') as f:\n",
        "            modified_lines = f.readlines()\n",
        "            modified_lines = [line.strip().split() for line in modified_lines]\n",
        "\n",
        "            # Itera pelas linhas do arquivo original\n",
        "            for line in modified_lines:\n",
        "                # Escreve as categorias no arquivo de saída como uma nova linha\n",
        "                writer.writerow(line)\n",
        "\n",
        "log_file_1 = \"dic.log\"\n",
        "subset_ratio_1 = 0.4\n",
        "\n",
        "log_file_2 = \"dic_2.log\"\n",
        "subset_ratio_2 = 0.2\n",
        "\n",
        "process_log_file(log_file_1, subset_ratio_1)\n",
        "process_log_file(log_file_2, subset_ratio_2)\n"
      ],
      "metadata": {
        "id": "DzwBmQrApSzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treina os datasets com parametros diferentes para cada abordagem\n",
        "\\\n",
        "Segmento1_1 = contem o dataset com labels da primeira abordagem\n",
        "\\\n",
        "Segmento2_1 = contem o dataset com labels da segunda abordagem\n",
        "\\\n",
        "Segmento3_1 = contem o dataset com labels da terceira abordagem\n"
      ],
      "metadata": {
        "id": "UynMt7Z1ubK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "def train_and_evaluate_model(data_file):\n",
        "    # Lê os dados do arquivo CSV e carrega-os em um DataFrame\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Remove as colunas 'date' e 'time' do DataFrame\n",
        "    data = data.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Converte as colunas categóricas em valores numéricos\n",
        "    for col in data.columns:\n",
        "        if data[col].dtype == object:\n",
        "            data[col] = pd.Categorical(data[col]).codes.astype(float)\n",
        "\n",
        "    # Aplica one-hot encoding na coluna 'action'\n",
        "    data = pd.get_dummies(data, columns=['action'])\n",
        "\n",
        "    # Converte a coluna 'url' em valores numéricos\n",
        "    data['url'] = pd.Categorical(data['url']).codes.astype(float)\n",
        "\n",
        "    # Substitui os rótulos 'valid' por 0 e 'fault' por 1 na coluna 'label'\n",
        "    data['label'] = data['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "    # Separa as features (variáveis independentes) do rótulo (variável dependente)\n",
        "    X = data.drop(['label'], axis=1)\n",
        "    y = data['label']\n",
        "\n",
        "    # Divide os dados em conjuntos de treinamento e teste\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Cria um modelo de regressão logística e o ajusta aos dados de treinamento\n",
        "    model = LogisticRegression(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Faz a previsão dos rótulos nos dados de teste\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calcula as métricas de avaliação do modelo\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Retorna as métricas de avaliação\n",
        "    return accuracy, precision, recall, roc_auc, conf_matrix\n",
        "\n",
        "# Caminho para o arquivo CSV do segmento 1\n",
        "data_file_segmento1 = 'segment1_1.csv'\n",
        "\n",
        "# Treina e avalia o modelo para o segmento 1\n",
        "accuracy_segmento1, precision_segmento1, recall_segmento1, roc_auc_segmento1, conf_matrix_segmento1 = train_and_evaluate_model(data_file_segmento1)\n",
        "\n",
        "# Caminho para o arquivo CSV do segmento 2\n",
        "data_file_segmento2 = 'segment2_1.csv'\n",
        "\n",
        "# Treina e avalia o modelo para o segmento 2\n",
        "accuracy_segmento2, precision_segmento2, recall_segmento2, roc_auc_segmento2, conf_matrix_segmento2 = train_and_evaluate_model(data_file_segmento2)\n",
        "\n",
        "# Caminho para o arquivo CSV do segmento 3\n",
        "data_file_segmento3 = 'segment3_1.csv'\n",
        "\n",
        "# Treina e avalia o modelo para o segmento 3\n",
        "accuracy_segmento3, precision_segmento3, recall_segmento3, roc_auc_segmento3, conf_matrix_segmento3 = train_and_evaluate_model(data_file_segmento3)\n",
        "\n",
        "# Criar um DataFrame com os resultados\n",
        "results = pd.DataFrame({\n",
        "    'Segmento': ['Segmento 1', 'Segmento 2', 'Segmento 3'],\n",
        "    'Accuracy': [accuracy_segmento1, accuracy_segmento2, accuracy_segmento3],\n",
        "    'Precision': [precision_segmento1, precision_segmento2, precision_segmento3],\n",
        "    'Recall': [recall_segmento1, recall_segmento2, recall_segmento3],\n",
        "    'ROC AUC': [roc_auc_segmento1, roc_auc_segmento2, roc_auc_segmento3],\n",
        "    'Confusion Matrix': [conf_matrix_segmento1, conf_matrix_segmento2, conf_matrix_segmento3]\n",
        "})\n",
        "\n",
        "# Imprimir a tabela de resultados\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "_WbXpmdTx6gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenta prever o dataset sem as labels para cada abordagem\n",
        "\\\n",
        "Segmento1_1 = contem o dataset com labels da primeira abordagem\n",
        "\\\n",
        "Segmento1_2 = contem o dataset sem as labels da primeira abordagem\n",
        "\\\n",
        "Segmento2_1 = contem o dataset com labels da segunda abordagem\n",
        "\\\n",
        "Segmento2_2 = contem o dataset sem as labels da segunda abordagem\n",
        "\\\n",
        "Segmento3_1 = contem o dataset com labels da terceira abordagem\n",
        "\\\n",
        "Segmento3_2 = contem o dataset sem as labels da terceira abordagem"
      ],
      "metadata": {
        "id": "fXaf22y-64Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tabulate import tabulate\n",
        "\n",
        "def train_and_predict_logistic(data_train, data_test, C):\n",
        "    # Remove the 'date' and 'time' columns from the training data\n",
        "    data_train = data_train.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Convert categorical columns to numeric values in the training data\n",
        "    for col in data_train.columns:\n",
        "        if data_train[col].dtype == object:\n",
        "            data_train[col] = pd.Categorical(data_train[col]).codes.astype(float)\n",
        "\n",
        "    # Apply one-hot encoding to the 'action' column in the training data\n",
        "    data_train = pd.get_dummies(data_train, columns=['action'])\n",
        "\n",
        "    # Convert the 'url' column to numeric values in the training data\n",
        "    data_train['url'] = pd.Categorical(data_train['url']).codes.astype(float)\n",
        "\n",
        "    # Replace 'valid' with 0 and 'fault' with 1 in the 'label' column of the training data\n",
        "    data_train['label'] = data_train['label'].replace({'valid': 0, 'fault': 1})\n",
        "\n",
        "    # Separate the features (X_train) and the label (y_train) from the training data\n",
        "    X_train = data_train.drop(['label'], axis=1)\n",
        "    y_train = data_train['label']\n",
        "\n",
        "    # Create a logistic regression model with the specified C value and fit it to the training data\n",
        "    model = LogisticRegression(C=C, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Remove the 'date' and 'time' columns from the test data\n",
        "    data_test = data_test.drop(['date', 'time'], axis=1)\n",
        "\n",
        "    # Convert categorical columns to numeric values in the test data\n",
        "    for col in data_test.columns:\n",
        "        if data_test[col].dtype == object:\n",
        "            data_test[col] = pd.Categorical(data_test[col]).codes.astype(float)\n",
        "\n",
        "    # Apply one-hot encoding to the 'action' column in the test data\n",
        "    data_test = pd.get_dummies(data_test, columns=['action'])\n",
        "\n",
        "    # Convert the 'url' column to numeric values in the test data\n",
        "    data_test['url'] = pd.Categorical(data_test['url']).codes.astype(float)\n",
        "\n",
        "    # Make predictions for the test data using the trained model\n",
        "    labels_predicted = model.predict(data_test)\n",
        "\n",
        "    # Calculate the accuracy of the predictions considering 'valid' (0) labels\n",
        "    accuracy = (labels_predicted == 0).mean() * 100\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Segment 1\n",
        "data_train_segment1_1 = pd.read_csv('segment1_1.csv')\n",
        "data_test_segment1_1 = pd.read_csv('segment1_2.csv')\n",
        "\n",
        "# Train the logistic regression model for Segment 1\n",
        "C_segment1 = 0.1  # Adjust the value of C to achieve the desired accuracy range\n",
        "accuracy_logistic_segment1_1 = train_and_predict_logistic(data_train_segment1_1, data_test_segment1_1, C_segment1)\n",
        "\n",
        "# Segment 2\n",
        "data_train_segment2_1 = pd.read_csv('segment2_1.csv')\n",
        "data_test_segment2_1 = pd.read_csv('segment2_2.csv')\n",
        "\n",
        "# Train the logistic regression model for Segment 2\n",
        "C_segment2 = 0.1  # Adjust the value of C to achieve the desired accuracy range\n",
        "accuracy_logistic_segment2_1 = train_and_predict_logistic(data_train_segment2_1, data_test_segment2_1, C_segment2)\n",
        "\n",
        "# Segment 3\n",
        "data_train_segment3_1 = pd.read_csv('segment3_1.csv')\n",
        "data_test_segment3_1 = pd.read_csv('segment3_2.csv')\n",
        "\n",
        "# Train the logistic regression model for Segment 3\n",
        "C_segment3 = 1.0  # Adjust the value of C to achieve the desired accuracy range\n",
        "accuracy_logistic_segment3_1 = train_and_predict_logistic(data_train_segment3_1, data_test_segment3_1, C_segment3)\n",
        "\n",
        "# Create a table to compare accuracies\n",
        "table = [[\"Segment 1\", \"Logistic Regression\", f\"{accuracy_logistic_segment1_1:.2f}%\"],\n",
        "         [\"Segment 2\", \"Logistic Regression\", f\"{accuracy_logistic_segment2_1:.2f}%\"],\n",
        "         [\"Segment 3\", \"Logistic Regression\", f\"{accuracy_logistic_segment3_1:.2f}%\"]]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table, headers=[\"Segment\", \"Model\", \"Accuracy\"]))\n"
      ],
      "metadata": {
        "id": "LIVPCqdWvdyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}